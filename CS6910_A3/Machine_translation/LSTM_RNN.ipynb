{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIbuu82Oowvs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "# nltk.download('punkt')\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_eng(text):\n",
        "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "# indic_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "def tokenize_hi(text):\n",
        "    return text.strip().split()  # crude word tokenizer for Hindi\n",
        "\n",
        "\n",
        "english = Field(tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "# We'll use raw text for Hindi target (tokenization handled separately)\n",
        "hindi = Field(tokenize=tokenize_hi, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "\n",
        "# Assume your CSV has columns: source, target\n",
        "train_data, valid_data = TabularDataset.splits(\n",
        "    path='.',  # directory where training.csv is located\n",
        "    train='/team7_hi/team7_hi_train.csv',\n",
        "    validation='/team7_hi/team7_hi_valid.csv',  # if you have a validation split, else ignore this line\n",
        "    format='csv',\n",
        "    fields=[('source', english), ('target', hindi)],\n",
        "    skip_header=True\n",
        ")\n",
        "test_data = TabularDataset(\n",
        "    path='/team7_hi/team7_hi_test.csv',\n",
        "    format='csv',\n",
        "    fields=[('source', english), ('target', hindi)],\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "# for i in range(50):\n",
        "#     print(vars(train_data.examples[i]))\n",
        "# Build vocab for English and Hindi on training data only\n",
        "english.build_vocab(train_data, min_freq=2)  # min_freq to avoid rare tokens\n",
        "hindi.build_vocab(train_data, min_freq=2)\n",
        "# with open(\"hindi_vocab_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     f.write(\"Index\\tToken\\n\")\n",
        "#     for idx, token in enumerate(hindi.vocab.itos):\n",
        "#         f.write(f\"{idx}\\t{token}\\n\")\n",
        "\n",
        "#     f.write(\"\\nToken\\tIndex\\n\")\n",
        "#     for token, idx in hindi.vocab.stoi.items():\n",
        "#         f.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def load_glove_embeddings(glove_path, word_to_idx, embedding_dim=200):\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word_to_idx), embedding_dim))\n",
        "    found = 0\n",
        "\n",
        "    with open(glove_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype=np.float32)\n",
        "            if word in word_to_idx:\n",
        "                embeddings[word_to_idx[word]] = vector\n",
        "                found += 1\n",
        "    print(f\"Found embeddings for {found} out of {len(word_to_idx)} words.\")\n",
        "    return embeddings\n",
        "\n",
        "# print(len(hindi.vocab.stoi))\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, num_layers, p, trainable=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = trainable  # Freeze embeddings (optional)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        return hidden, cell\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_indicbert_embedding_matrix(vocab_stoi, model_name=\"ai4bharat/indic-bert\"):\n",
        "    \"\"\"\n",
        "    Returns a numpy array embedding matrix for your vocabulary aligned to indic-bert embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab_stoi: dict mapping token string to index (e.g., hindi.vocab.stoi)\n",
        "        model_name: pretrained IndicBERT model name\n",
        "\n",
        "    Returns:\n",
        "        embedding_matrix: numpy array of shape (vocab_size, embedding_dim)\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Use actual embedding dim from the model, not model.config.hidden_size\n",
        "    embedding_layer = model.get_input_embeddings()\n",
        "    embedding_dim = embedding_layer.embedding_dim\n",
        "    print(\"Using embedding_dim =\", embedding_dim)\n",
        "\n",
        "    vocab_size = len(vocab_stoi)\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
        "    print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for token, idx in vocab_stoi.items():\n",
        "            tokenized = tokenizer.tokenize(token)\n",
        "            if len(tokenized) == 0:\n",
        "                tokenized = [tokenizer.unk_token]\n",
        "\n",
        "            token_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "            token_ids_tensor = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get subword embeddings and average them\n",
        "            embeddings = embedding_layer(token_ids_tensor)  # (1, subword_count, embedding_dim)\n",
        "            avg_embedding = embeddings.mean(dim=1).squeeze(0)  # (embedding_dim,)\n",
        "\n",
        "            embedding_matrix[idx] = avg_embedding.cpu().numpy()\n",
        "\n",
        "    return embedding_matrix\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, embedding_matrix, hidden_size, output_size, num_layers, p, trainable=False\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        vocab_size, embedding_size = embedding_matrix.shape\n",
        "\n",
        "        # Use pretrained embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.tensor(embedding_matrix), freeze=not trainable\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x shape: (N,) â†’ batch of indices\n",
        "        x = x.unsqueeze(0)  # shape: (1, N)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs.squeeze(0))  # shape: (N, output_size)\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "embedding_matrix_hindi = get_indicbert_embedding_matrix(hindi.vocab.stoi, model_name=\"ai4bharat/indic-bert\")\n",
        "embedding_matrix_english = load_glove_embeddings('glove.6B.200d.txt', english.vocab.stoi, 200)\n",
        "\n",
        "encoder = Encoder(embedding_matrix=embedding_matrix_english, hidden_size=256, num_layers=2, p=0.5)\n",
        "decoder = Decoder(embedding_matrix=embedding_matrix_hindi, hidden_size=256, output_size=len(hindi.vocab), num_layers=2, p=0.5, trainable=False)\n",
        "\n",
        "print(embedding_matrix_english.shape)\n",
        "print(embedding_matrix_hindi.shape)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(hindi.vocab)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            # print(best_guess)\n",
        "            # print(hindi.vocab.itos[best_guess])\n",
        "            # predicted_words = [hindi.vocab.itos[idx.item()] for idx in best_guess]\n",
        "            # print(predicted_words)\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Vocabulary sizes\n",
        "input_size_encoder = len(english.vocab)\n",
        "output_size_decoder = len(hindi.vocab)\n",
        "\n",
        "# Embedding matrices (assumed preloaded)\n",
        "# embedding_matrix_english: torch.FloatTensor with GloVe vectors\n",
        "# embedding_matrix_hindi: torch.FloatTensor with IndicBERT vectors\n",
        "\n",
        "# Hyperparameters\n",
        "HIDDEN_SIZE = 1024\n",
        "NUM_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Dataset iterators (replace with your actual dataset and Field objects)\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.source),\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Encoder and Decoder using preloaded embedding matrices\n",
        "encoder = Encoder(\n",
        "    embedding_matrix=embedding_matrix_english,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    p=ENC_DROPOUT,\n",
        "    trainable=False\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    embedding_matrix=embedding_matrix_hindi,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    output_size=output_size_decoder,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    p=DEC_DROPOUT,\n",
        "    trainable=False\n",
        ").to(device)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# # Optimizer and Loss\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# pad_idx = hindi.vocab.stoi[\"<pad>\"]\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "\n",
        "# # Early stopping params\n",
        "# patience = 5\n",
        "# best_valid_loss = float('inf')\n",
        "# early_stop_counter = 0\n",
        "\n",
        "# train_losses = []\n",
        "# valid_losses = []\n",
        "\n",
        "# # Fixed sentence to translate\n",
        "# fixed_sentence = \"the weather is nice today\"\n",
        "# translation_log_file = \"translation_progress.txt\"\n",
        "\n",
        "# with open(translation_log_file, \"w\", encoding=\"utf-8\") as f_log:\n",
        "#     f_log.write(\"Translation Progress Over Epochs\\n\\n\")\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(f\"[Epoch {epoch+1}/{num_epochs}]\")\n",
        "\n",
        "#     model.train()\n",
        "#     epoch_loss = 0\n",
        "\n",
        "#     for batch in train_iterator:\n",
        "#         src = batch.source.to(device)\n",
        "#         trg = batch.target.to(device)\n",
        "\n",
        "#         output = model(src, trg)\n",
        "\n",
        "#         output = output[1:].reshape(-1, output.shape[2])\n",
        "#         trg = trg[1:].reshape(-1)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = criterion(output, trg)\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "#         optimizer.step()\n",
        "\n",
        "#         epoch_loss += loss.item()\n",
        "\n",
        "#     train_loss = epoch_loss / len(train_iterator)\n",
        "#     train_losses.append(train_loss)\n",
        "\n",
        "#     # Validation loss\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in valid_iterator:\n",
        "#             src = batch.source.to(device)\n",
        "#             trg = batch.target.to(device)\n",
        "#             output = model(src, trg, 0)  # No teacher forcing in validation\n",
        "\n",
        "#             output = output[1:].reshape(-1, output.shape[2])\n",
        "#             trg = trg[1:].reshape(-1)\n",
        "\n",
        "#             loss = criterion(output, trg)\n",
        "#             val_loss += loss.item()\n",
        "\n",
        "#     valid_loss = val_loss / len(valid_iterator)\n",
        "#     valid_losses.append(valid_loss)\n",
        "\n",
        "#     print(f\"Train Loss: {train_loss:.4f} | Val Loss: {valid_loss:.4f}\")\n",
        "\n",
        "#     # Early stopping logic\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         early_stop_counter = 0\n",
        "#         torch.save(model.state_dict(), \"best_model.pt\")  # optional: save best model\n",
        "#     else:\n",
        "#         early_stop_counter += 1\n",
        "#         if early_stop_counter >= patience:\n",
        "#             print(\"Early stopping triggered.\")\n",
        "#             break\n",
        "\n",
        "#     # Translate fixed sentence after each epoch\n",
        "#     model.eval()\n",
        "#     tokens = [tok.text.lower() for tok in spacy_eng.tokenizer(fixed_sentence)]\n",
        "#     tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
        "#     indices = [english.vocab.stoi[token] if token in english.vocab.stoi else english.vocab.stoi[\"<unk>\"] for token in tokens]\n",
        "#     src_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)  # shape: (src_len, 1)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         hidden, cell = model.encoder(src_tensor)\n",
        "#         x = torch.LongTensor([hindi.vocab.stoi[\"<sos>\"]]).to(device)\n",
        "#         translated_sentence = []\n",
        "\n",
        "#         for _ in range(50):  # max length\n",
        "#             output, hidden, cell = model.decoder(x, hidden, cell)\n",
        "#             best_guess = output.argmax(1).item()\n",
        "#             predicted_word = hindi.vocab.itos[best_guess]\n",
        "\n",
        "#             if predicted_word == \"<eos>\":\n",
        "#                 break\n",
        "#             translated_sentence.append(predicted_word)\n",
        "#             x = torch.LongTensor([best_guess]).to(device)\n",
        "\n",
        "#     translated_text = \" \".join(translated_sentence)\n",
        "#     with open(translation_log_file, \"a\", encoding=\"utf-8\") as f_log:\n",
        "#         f_log.write(f\"Epoch {epoch+1}: {translated_text}\\n\")\n",
        "\n",
        "# # Plot training and validation loss\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(train_losses, label=\"Training Loss\")\n",
        "# plt.plot(valid_losses, label=\"Validation Loss\")\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training vs Validation Loss\")\n",
        "# plt.legend()\n",
        "# plt.savefig(\"loss_curve.png\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # Load the trained model weights\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [tok.text.lower() for tok in spacy_eng.tokenizer(sentence)]\n",
        "    else:\n",
        "        tokens = [tok.lower() for tok in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "\n",
        "    src_indices = [src_field.vocab.stoi.get(tok, src_field.vocab.stoi[src_field.unk_token]) for tok in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indices = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indices[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        best_guess = output.argmax(1).item()\n",
        "        trg_indices.append(best_guess)\n",
        "\n",
        "        if best_guess == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    translated_tokens = [trg_field.vocab.itos[idx] for idx in trg_indices[1:]]  # remove <sos>\n",
        "    return translated_tokens\n",
        "\n",
        "\n",
        "# # Evaluate BLEU on a dataset iterator\n",
        "# def evaluate_bleu(model, iterator, src_field, trg_field, device):\n",
        "#     trgs = []\n",
        "#     preds = []\n",
        "\n",
        "#     for batch in iterator:\n",
        "#         src = getattr(batch, 'src', getattr(batch, 'source', None))\n",
        "#         trg = getattr(batch, 'trg', getattr(batch, 'target', None))\n",
        "\n",
        "#         src = src.transpose(0, 1)  # shape: (batch_size, src_len)\n",
        "#         trg = trg.transpose(0, 1)  # shape: (batch_size, trg_len)\n",
        "\n",
        "#         for i in range(src.shape[0]):\n",
        "#             src_indices = src[i].tolist()\n",
        "#             trg_indices = trg[i].tolist()\n",
        "\n",
        "#             src_tokens = [src_field.vocab.itos[idx] for idx in src_indices if idx not in\n",
        "#                           [src_field.vocab.stoi[tok] for tok in [src_field.pad_token, src_field.init_token, src_field.eos_token]]]\n",
        "\n",
        "#             trg_tokens = [trg_field.vocab.itos[idx] for idx in trg_indices if idx not in\n",
        "#                           [trg_field.vocab.stoi[tok] for tok in [trg_field.pad_token, trg_field.init_token, trg_field.eos_token]]]\n",
        "\n",
        "#             pred_tokens = translate_sentence(src_tokens, src_field, trg_field, model, device)\n",
        "\n",
        "#             preds.append(pred_tokens)\n",
        "#             trgs.append([trg_tokens])  # wrap target in a list for BLEU format\n",
        "\n",
        "#     bleu1 = bleu_score(preds, trgs, max_n=1, weights=[1.0])\n",
        "#     bleu2 = bleu_score(preds, trgs, max_n=2, weights=[0.5, 0.5])\n",
        "#     bleu3 = bleu_score(preds, trgs, max_n=3, weights=[1/3, 1/3, 1/3])\n",
        "#     bleu4 = bleu_score(preds, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "\n",
        "#     print(f\"BLEU-1: {bleu1 * 100:.2f}\")\n",
        "#     print(f\"BLEU-2: {bleu2 * 100:.2f}\")\n",
        "#     print(f\"BLEU-3: {bleu3 * 100:.2f}\")\n",
        "#     print(f\"BLEU-4: {bleu4 * 100:.2f}\")\n",
        "\n",
        "#     return bleu1, bleu2, bleu3, bleu4\n",
        "\n",
        "import random\n",
        "\n",
        "def print_random_translations_to_file(dataset, src_field, trg_field, model, device, output_path, n=5, name=\"Dataset\"):\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Translations from {name}:\\n\")\n",
        "\n",
        "        examples = random.sample(dataset.examples, n)\n",
        "\n",
        "        for i, example in enumerate(examples):\n",
        "            src_tokens = getattr(example, 'src', getattr(example, 'source', None))\n",
        "            trg_tokens = getattr(example, 'trg', getattr(example, 'target', None))\n",
        "\n",
        "            if isinstance(src_tokens, list):\n",
        "                src_text = ' '.join(src_tokens)\n",
        "            else:\n",
        "                src_text = ' '.join([src_field.vocab.itos[idx] for idx in src_tokens])\n",
        "\n",
        "            if isinstance(trg_tokens, list):\n",
        "                trg_text = ' '.join(trg_tokens)\n",
        "            else:\n",
        "                trg_text = ' '.join([trg_field.vocab.itos[idx] for idx in trg_tokens])\n",
        "\n",
        "            translation = translate_sentence(src_text, src_field, trg_field, model, device)\n",
        "            translation_text = ' '.join(translation).replace('<eos>', '').strip()\n",
        "\n",
        "            f.write(f\"\\nExample {i+1}:\\n\")\n",
        "            f.write(f\"  Source:      {src_text}\\n\")\n",
        "            f.write(f\"  Target:      {trg_text}\\n\")\n",
        "            f.write(f\"  Translation: {translation_text}\\n\")\n",
        "\n",
        "print_random_translations_to_file(train_data, english, hindi, model, device, \"train_translations.txt\", n=5, name=\"Train Data\")\n",
        "print_random_translations_to_file(test_data, english, hindi, model, device, \"test_translations.txt\", n=5, name=\"Test Data\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Run BLEU evaluation\n",
        "# print(\"Evaluating on TRAIN set...\")\n",
        "# evaluate_bleu(model, train_iterator, english, hindi, device)\n",
        "\n",
        "# print(\"\\nEvaluating on TEST set...\")\n",
        "# evaluate_bleu(model, test_iterator, english, hindi, device)\n",
        "\n"
      ]
    }
  ]
}